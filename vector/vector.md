# ベクトルの概要についてまとめよ

ベクトルとは、「空間における大きさと方向を持った量」の事を言います。
またベクトルは数字を並べられたもので、縦に並べられたものを **「縦ベクトル」・「列ベクトル」**　と呼び、横に並べられたものを **「横ベクトル」・「行ベクトル」** となります。

例えばp→(3,1)とは図のようになります。
[別途資料,図１を参照ください]

ベクトル空間のベクトルに対比するものとしての実数を **スカラー** と呼び、ベクトルを定数倍して別のベクトルを作り出す演算として **スカラー倍** が定義されます。
[例文1を参照]

またベクトルの **内積** は、2つのベクトルが、どのくらい同じ向きを向いているかを表します。
ベクトルの内積は、ベクトルの内積が小さくなるほど、ベクトルの向きは反対とり、ベクトルの内積が大きくなるほど、ベクトルの向きは同じになります。

例えばp→(a,b)とq→(x,y)の内積を求めるとき、ベクトルの内積の定義は、

p→・q→＝ax+by　と定義します。
p→とq→の第１成分どうし、第２成分どうしを掛けて和をとります。これが内積の定義です。

例を見てみましょう。

[図２を参照ください]

それぞれの内積は先ほどのベクトルの定義により、
p.q = 3*-2+3*2 = 0
p.r = 3*-2+3*-2 = -12

ベクトルの向きが似ているpとqの方が内積が大きくなっており、ベクトルの性質をうまく表していることが分かります。

ノルムは、平面あるいは空間におけるベクトルの "長さ" であり、ベクトル空間に対して「距離」を与えるためのものです。
またノルムが１のベクトルのことを **単位ベクトル** と言います。

また2つのベクトル a→ と b→ は，内積 a→・b→＝0 のときに **直交(垂直になること)** するという。

固有値、固有ベクトルとは？

n次正方行列Aについて、
Ax = λx，x ≠ 0
のとき、行列Aをかけると、長さがλ倍になるようなx
の事を **固有ベクトル** , λを **固有値** と言います。

# ベクトルの概要について素人にも分かるように簡潔に説明せよ

日常の会話で「貴方とはベクトルが違う」などといったベクトルという言葉を使うことがあります。
それはまさに数学のベクトルから来ています。
一体ベクトルとはどういった意味でしょう？
きっと **「方向性」** といった意味で使われている方が多いと思います。

まさに数学におけるベクトルも、**方向性** などの情報を持った力の事を言います。
一体どういった意味かといいますと、A地点からB地点まで歩くとすれば、そのABの距離（長さ）がベクトルの大きさとなり、向かう方向がベクトルの向きとなるそういった情報を持つものが **ベクトル** とイメージして頂ければ分かりやすいのではないでしょうか？

# 行列の概要についてまとめよ

行列とは、上記で説明した列ベクトルと行ベクトルを合わせたものを **行列** と言います。

行列は計算することができ、二つの行列を足すことを **行列の和** 掛け合わせたものを **行列の積** と言います。
また行列の積は行ベクトルの要素の個数(列数)と
列ベクトルの要素の個数(行数)が **等しい** ときにしか計算することができません。
[例２を参照]

行の数と列の数が一致している行列のことを **正方行列** と言います。
そのサイズが[n×n型]の時、 **n次正方行列** と言います。

正則行列に掛け合わせるとEになる(AB = BA = E)行列を **逆行列** と言いA−1で表します。要は正則行列の相方です。
[例３を参照]

行列Aの行と列を入れ替えたものをAの **転置行列** と言い、ATと書きます。
行と列をひっくり返したものをまたひっくり返すので、元に戻ります。
[例４を参照]

また **対称行列** は、自身の転置行列と一致するような正方行列を言う。
記号で書けば、行列 A は A = AT を満たすとき対称であるという。
さらに転置行列と逆行列が等しくなる正方行列のことを　**直交行列** という。
つまりn × n の行列 A の転置行列を AT と表すときに、ATA = AAT = E を満たすようなAのこと。

他にも正方行列であって、その対角成分（(i, i)-要素）以外が0であるような行列のことを **対角行列** という。
[例5を参照]

**固有値分解** について
対称行列 A を変換する縦ベクトル𝑣について、このようなベクトルを考える。 A𝑣 = λ𝑣 ここでの　λ はスカラーである。A と内積をとることで、そのベクトル𝑣 のスカラー倍にできるべクトルである。
また固有値分解・特異値分解（SVD）は機械学習などのデータ圧縮(次元削減)などで用いられています。

# 行列の概要について素人にも分かるように簡潔に説明せよ

行列とは簡単にいうと「データの集合体」です。
例えばAさんの身長、体重、年齢などの情報を、一つの箱に入れその箱の集合とイメージしていただければ分かりやすいのではないでしょうか？
ベクトルと行列を使うことで、大量のデータの計算がしやすくなるので、データサイエンスの世界では必要不可欠となります。

# 線形代数の機械学習、深層学習における使用についてまとめよ（理解を採点者に伝える）

なぜ機械学習や深層学習で線形代数が必要になるかと言いますと、機械学習での解決方法を定式化する際に、線形代数の記法を使わない時よりも、だいぶ簡潔に記述することができます。

機械学習では同時に大量のデータを処理し、分類する必要があります。
特に教師あり学習や教師なし学習の分類では、たくさんのデータを仲間分けする場面が度々あります。この時、それぞれのデータを扱う式を一つにまとめて簡単に扱うために線形代数が必要不潔となります。

また機械学習や深層学習では、行列の重みを格納します。線形代数は、特にGPUのトレーニング時の行列演算を高速かつ簡単にしてくれます。
実は、GPUはベクトルと行列の演算を念頭に置いて作られたのです。
GPUは、ピクセルを1つずつ処理するのではなく、ピクセルの行列全体を並列処理してくれます。

# 線形代数が機械学習、深層学習における使用について素人にも分かるように説明せよ

まず機械学習や深層学習は、大量のデータを処理、計算することによって学習していきます。
例えば、健康状態を予測する為に身長、体重、年齢などのデータを処理するとして、１人だけを処理するのであれば線形代数を使わずともすぐに処理できますが、機械学習の世界では、多いときは何万人分のデータを処理しなければならない場合もあります。
そのような時に線形代数を使い、効率的に計算をして莫大なデータを処理します。

# 固有値と固有ベクトル 問題
probrem1~3の画像データに記入しました。
